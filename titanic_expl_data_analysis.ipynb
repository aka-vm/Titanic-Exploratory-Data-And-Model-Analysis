{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVIVAL_LABEL = [\"Survived\", \"Deceased\"]\n",
    "\n",
    "PASSENGERID = \"PassengerId\"\n",
    "SURVIVED    = \"Survived\"\n",
    "PCLASS      = \"Pclass\"\n",
    "NAME        = \"Name\"\n",
    "SEX         = \"Sex\"\n",
    "AGE         = \"Age\"\n",
    "SIBSP       = \"SibSp\"\n",
    "PARCH       = \"Parch\"\n",
    "TICKET      = \"Ticket\"\n",
    "FARE        = \"Fare\"\n",
    "CABIN       = \"Cabin\"\n",
    "EMBARKED    = \"Embarked\"\n",
    "\n",
    "AGE_RANGE       = \"Age_Range\"\n",
    "GENDER_AGE_CAT  = \"Gender_Age_Cat\"\n",
    "IS_ALONE        = \"Is_Alone\"\n",
    "AGE_PREDICT     = \"Age_Predict\"\n",
    "\n",
    "FEATURES_1          = [SURVIVED ,PCLASS ,SEX ,AGE ,SIBSP ,PARCH ,FARE ,EMBARKED]\n",
    "FEATURES_2          = [GENDER_AGE_CAT, IS_ALONE, AGE_PREDICT]\n",
    "FEATURES_USELESS    = [PASSENGERID, TICKET, CABIN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"data/test.csv\"\n",
    "train_data_path = \"data/train.csv\"\n",
    "submission_data_path = \"data/gender_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_data_path)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_data_path)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [df_train, df_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat(datasets).reset_index(drop=True)\n",
    "\n",
    "m = df_full.shape[0]\n",
    "m_test  = df_test.shape[0]\n",
    "m_train = df_train.shape[0]\n",
    "print(m, m_train, m_test)\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_devide(df_: pd.DataFrame) -> tuple[pd.DataFrame]:\n",
    "    df_train = df_[(df_full.Survived.notna())]\n",
    "    df_test = df_[(df_full.Survived.isna())]\n",
    "    return df_train, df_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[IS_ALONE] = ((df_full.Parch + df_full.SibSp) == 0).astype(int)\n",
    "\n",
    "df_full[IS_ALONE].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Using Weak Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "_d_ = df_train[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"SibSp\", \"Parch\"]].copy()\n",
    "_d_ = _d_.dropna()\n",
    "\n",
    "_encoder_ = LabelEncoder()\n",
    "_d_.iloc[:, 2] = _encoder_.fit_transform(_d_.iloc[:, 2].values)\n",
    "_d_.iloc[:, 5] = _encoder_.fit_transform(_d_.iloc[:, 5].values)\n",
    "\n",
    "_s_scaler_ = StandardScaler()\n",
    "_d_[[\"Age\", \"Fare\"]] = _s_scaler_.fit_transform(_d_[[\"Age\", \"Fare\"]])\n",
    "\n",
    "_x_ = _d_.iloc[:, 1:].values\n",
    "_y_ = _d_.Survived.values\n",
    "\n",
    "_weak_model_ = LogisticRegression().fit(_x_, _y_)\n",
    "\n",
    "_coef_ = _weak_model_.coef_.round(4).tolist()\n",
    "\n",
    "sorted(\n",
    "    list(zip(_d_.columns[1:], _coef_[0][1:]))\n",
    "    , key=lambda tup: abs(tup[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here It can be been that which labels had big role so lets focus on them first..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up visualisations\n",
    "sns.set_style(style='white') \n",
    "sns.set(rc={\n",
    "    'figure.figsize':(10,6), \n",
    "    'axes.facecolor': '#eee',\n",
    "    'axes.grid': True,\n",
    "    'grid.color': '.9',\n",
    "    'axes.linewidth': 1.0,\n",
    "    'grid.linestyle': u'-'},font_scale=1)\n",
    "custom_colors = [\"#3498db\", \"#95a5a6\",\"#34495e\", \"#2ecc71\", \"#e74c3c\"]\n",
    "sns.set_palette(custom_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Survival Status / Various Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Survived vs Deceased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_ratio = df_full.Survived.value_counts(normalize=True)\n",
    "survival_ratio.plot.barh(color=[\"C1\", \"C0\"],)\n",
    "\n",
    "plt.yticks((1, 0), labels=SURVIVAL_LABEL)\n",
    "plt.title(\"Survived vs Deceased\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(22,10)})\n",
    "\n",
    "# ax = sns.countplot(y=\"answer\", hue=\"sex\", data=df)\n",
    "\n",
    "# # percentage of bars\n",
    "# for i in ax.patches:\n",
    "#     # get_width pulls left or right; get_y pushes up or down\n",
    "#     ax.text(i.get_width()+.12, i.get_y()+.3, \\\n",
    "#             '%' + str(round((i.get_width()/total)*100, 1)), fontsize=15,\n",
    "#             color='dimgrey')\n",
    "    \n",
    "# ax.set_ylabel('Answers',fontsize=20)\n",
    "# ax.set_xlabel('Count',fontsize=20)\n",
    "# ax.tick_params(axis='x', which='major', labelsize=20)\n",
    "# ax.tick_params(axis='y', which='major', labelsize=20)\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.,\n",
    "#           prop={'size': 14})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Survival / Sex (Male vs Female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_survival_dist_chart = sns.countplot(x=SEX, hue=SURVIVED, data=df_full, palette=[\"C1\", \"C0\"])\n",
    "sex_survival_dist_chart.set(ylabel=\"Percent\")\n",
    "\n",
    "plt.title(\"Male vs Female Survival\")\n",
    "plt.legend(SURVIVAL_LABEL[::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_survived_dist    = df_full[df_full.Survived==1].Sex.value_counts().sort_index()\n",
    "sex_deceased_dist    = df_full[df_full.Survived==0].Sex.value_counts().sort_index()\n",
    "sex_survival_dist    = pd.DataFrame([sex_survived_dist, sex_deceased_dist], index=SURVIVAL_LABEL).T\n",
    "sex_survival_ratio   = sex_survival_dist.apply(lambda x: x/x.sum(), axis=1)\n",
    "\n",
    "sex_survival_ratio.plot.bar()\n",
    "plt.title(\"Survival Ratio / Sex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that Sex Really affected the Chance of Survival.<br>\n",
    "The Bars are almost inverse of each other.\n",
    "\n",
    "* Female - Lucky\n",
    "* Male - Unlucky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Survival / Pclass Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass_survived_dist    = df_full[df_full.Survived==1].Pclass.value_counts().sort_index()\n",
    "pclass_deceased_dist    = df_full[df_full.Survived==0].Pclass.value_counts().sort_index()\n",
    "pclass_survival_dist    = pd.DataFrame([pclass_survived_dist, pclass_deceased_dist], index=SURVIVAL_LABEL).T\n",
    "pclass_survival_ratio   = pclass_survival_dist.apply(lambda x: x/x.sum(), axis=1)\n",
    "\n",
    "pclass_survival_ratio.plot.bar()\n",
    "plt.title(\"Survival Ratio / Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that People with **higher Pclass** had more chance of survival than the people with **Lower Pclass** .<br>\n",
    "Luck-> 1>2>3\n",
    "\n",
    "* Pclass 1 - >60% Survival Change\n",
    "* Pclass 2 - <50% Survival Change\n",
    "* Pclass 3 - =25% Survival Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Survival / Age Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_full.Age, bins=30)\n",
    "plt.title(\"Age Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bins = np.arange(0, 81, 10)\n",
    "Age_Range_df = pd.cut(df_full.Age, bins=age_bins, include_lowest=True)\n",
    "\n",
    "age_grp_survived_dist    = Age_Range_df[df_full.Survived==1].value_counts().sort_index()\n",
    "age_grp_deceased_dist    = Age_Range_df[df_full.Survived==0].value_counts().sort_index()\n",
    "age_grp_survival_dist    = pd.DataFrame([age_grp_survived_dist, age_grp_deceased_dist], index=SURVIVAL_LABEL).T\n",
    "age_grp_survival_ratio   = age_grp_survival_dist.apply(lambda x: x/x.sum(), axis=1)\n",
    "\n",
    "age_grp_survival_ratio.plot.bar()\n",
    "plt.title(\"Survival Ratio / Age-Group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the Chance of Survival is different in different Age-Groups.<br>\n",
    "The Bars are mixed, but we can see that - \n",
    "* 0-10 yrs agr group has the Highest chance of survival.\n",
    "* 70-80 yrs agr group has the Lowest chance of survival.\n",
    "* The Chance of Survial Varies from 60% - 20% depending upon the Age Group...\n",
    "\n",
    "Thus The Age of the Passenger is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Survival / Sibbling & Spouse Number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sibsp_survived_dist    = df_full[df_full.Survived==1].SibSp.value_counts().sort_index()\n",
    "sibsp_deceased_dist    = df_full[df_full.Survived==0].SibSp.value_counts().sort_index()\n",
    "sibsp_survival_dist    = pd.DataFrame([sibsp_survived_dist, sibsp_deceased_dist], index=SURVIVAL_LABEL).T\n",
    "sibsp_survival_ratio   = sibsp_survival_dist.apply(lambda x: x/x.sum(), axis=1)\n",
    "\n",
    "sibsp_survival_ratio.plot.bar()\n",
    "plt.title(\"Survival Ratio / Sibbling & Spouse number \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the Chance of Survival is increased if the passengers is traveling with 1-2 of their Sibling/Spouse then redused after that.<br>\n",
    "\n",
    "The Bars follow - \n",
    "* mid-up-mid-down pattern is followed.\n",
    "* Having >2 Sibling/Spouse slump the Chance of Survival.\n",
    "* The Chance of Survial is best with 1-2 Sibling/Spouse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Survival / Parents & Children Number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parch_survived_dist    = df_full[df_full.Survived==1].Parch.value_counts().sort_index()\n",
    "parch_deceased_dist    = df_full[df_full.Survived==0].Parch.value_counts().sort_index()\n",
    "parch_survival_dist    = pd.DataFrame([parch_survived_dist, parch_deceased_dist], index=SURVIVAL_LABEL).T\n",
    "parch_survival_ratio   = parch_survival_dist.apply(lambda x: x/x.sum(), axis=1)\n",
    "\n",
    "parch_survival_ratio.plot.bar()\n",
    "plt.title(\"Survival Ratio / Parents & Children Number \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to SibSp, here we can see that the Chance of Survival is increased if the passengers is traveling with 1-3 of their Parents/Children then redused after that.<br>\n",
    "\n",
    "The Bars follow - \n",
    "* Pattern similar to SibSp\n",
    "* Having >3 Parents/Children slump the Chance of Survival.\n",
    "* The Chance of Survial is best with 1-3 Parents/Children.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Survival / Family Members Number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_num_df = df_full.Parch + df_full.SibSp\n",
    "\n",
    "fam_num_survived_dist    = fam_num_df[df_full.Survived==1].value_counts().sort_index()\n",
    "fam_num_deceased_dist    = fam_num_df[df_full.Survived==0].value_counts().sort_index()\n",
    "fam_num_survival_dist    = pd.DataFrame([fam_num_survived_dist, fam_num_deceased_dist], index=SURVIVAL_LABEL).T\n",
    "fam_num_survival_ratio   = fam_num_survival_dist.apply(lambda x: x/x.sum(), axis=1)\n",
    "\n",
    "fam_num_survival_ratio.plot.bar()\n",
    "plt.title(\"Survival Ratio / Family Members Number \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add the number of family members we get a really intresting pattern.<br>\n",
    "\n",
    "The Bars follow - \n",
    "* Mid-Up-Down pattern, with less maximas and minimas.\n",
    "* Having >3 Family Members slump the Chance of Survival.\n",
    "* The Chance of Survial is best with 1-3 Family Members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Survival / Embark Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embark_survived_dist    = df_full[df_full.Survived==1].Embarked.value_counts().sort_index()\n",
    "embark_deceased_dist    = df_full[df_full.Survived==0].Embarked.value_counts().sort_index()\n",
    "embark_survival_dist    = pd.DataFrame([embark_survived_dist, embark_deceased_dist], index=SURVIVAL_LABEL).T\n",
    "embark_survival_ratio   = embark_survival_dist.apply(lambda x: x/x.sum(), axis=1)\n",
    "\n",
    "embark_survival_ratio.plot.bar()\n",
    "plt.title(\"Survival Ratio / Embarkment Point \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaguely Speaking, The Embarkment Point dosen't seem to be a big reason for variation in survival rate. But Visually it says a different story.\n",
    "<br>\n",
    "\n",
    "People who embarked from C, have Higher Chance of survival Than those who Embarked from Other Places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Survival / Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.Fare.describe().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_bins = [0, 8, 14, 31, 513]\n",
    "fare_Range_df = pd.cut(df_full.Fare, bins=fare_bins, include_lowest=True)\n",
    "\n",
    "fare_grp_survived_dist    = fare_Range_df[df_full.Survived==1].value_counts().sort_index()\n",
    "fare_grp_deceased_dist    = fare_Range_df[df_full.Survived==0].value_counts().sort_index()\n",
    "fare_grp_survival_dist    = pd.DataFrame([fare_grp_survived_dist, fare_grp_deceased_dist], index=SURVIVAL_LABEL).T\n",
    "fare_grp_survival_ratio   = fare_grp_survival_dist.apply(lambda x: x/x.sum(), axis=1)\n",
    "\n",
    "fare_grp_survival_ratio.plot.bar()\n",
    "plt.title(\"Survival Ratio / Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(df_full.Fare.dropna() + 1e-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_Range_df = np.log(df_full.Fare.dropna() + 1e-1).astype(int)\n",
    "\n",
    "fare_log_grp_survived_dist    = fare_Range_df[df_full.Survived==1].value_counts().sort_index()\n",
    "fare_log_grp_deceased_dist    = fare_Range_df[df_full.Survived==0].value_counts().sort_index()\n",
    "fare_log_grp_survival_dist    = pd.DataFrame([fare_log_grp_survived_dist, fare_log_grp_deceased_dist], index=SURVIVAL_LABEL).T\n",
    "fare_log_grp_survival_ratio   = fare_log_grp_survival_dist.apply(lambda x: x/x.sum(), axis=1)\n",
    "\n",
    "fare_log_grp_survival_ratio.plot.bar()\n",
    "plt.title(\"Survival Ratio / Fare Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare is related to Pclass, Higher the Fare, Higher The Class.<br>\n",
    "It is a Greate Feature to Use insted/along Pclass...<br><br>\n",
    "\n",
    "It will be tested later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engneering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Dataset and Dealing With it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=df_full.notna(), cbar=False, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data with \"White\" lines represent missing data and \"Blue\" lines represent not-null data.<br>\n",
    "We can either predict the missing data or leave the entire feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k : i \n",
    " for k, i in (m - df_full.notna().sum()).to_dict().items()\n",
    " if i}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Now Predict the missing Values of **Embarked**, **Fare** and **Age**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.Embarked.value_counts(normalize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since around 70% people Embarked from S.\n",
    "lets simply take the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.Embarked = df_full.Embarked.fillna(df_full.Embarked.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.Embarked.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[df_full.Fare.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be choosing the mean of Fare in the group of people whose Pclass and Sex are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_1 = df_full.groupby([PCLASS, SEX, IS_ALONE])\n",
    "df_full.Fare = grp_1.Fare.apply(lambda fare: fare.fillna(fare.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Age Using only Sex and PClass may not be a good way, as this would easily skip all the children since the mean age is so high.<br>\n",
    "\n",
    "Insted Lets Use Name. Yes, Name! to predict the age. <br>\n",
    "\n",
    "We can use the title in the name as one of the feature to predict the age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df_full.Name.apply(lambda name: name.split(\",\")[1].split(\".\")[0].strip())\n",
    "print(titles.unique())\n",
    "titles[df_full.Age.isna()].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Can join and use some titles as one. <br>\n",
    "Though, This will increase the number of groups, at the same time this will also reduce edge case percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_titles     = titles[(df_full.Sex == \"male\")].unique()\n",
    "female_titles   = titles[(df_full.Sex == \"female\")].unique()\n",
    "\n",
    "unisex_titles       = [title_ for title_ in male_titles if title_ in female_titles]\n",
    "male_only_titles    = [title_ for title_ in male_titles if title_ not in unisex_titles]\n",
    "female_only_titles  = [title_ for title_ in female_titles if title_ not in unisex_titles]\n",
    "\n",
    "unisex_titles, male_only_titles, female_only_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make 5 Categories - \n",
    "* Mr,  Don,  Rev,  Major,  Sir,  Col,  Capt,  Jonkheer, [Dr]\n",
    "* Mrs,  Mme,  Lady,  Mlle,  the Countess,  Dona, [Dr]\n",
    "* Master\n",
    "* Miss, Ms (Those Who Traveled Alone, Implying They may be olded)\n",
    "* Miss, Ms (Those Who Traveled With Someone, Implying They may be younger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_Cat_1 = [\"Mr\",  \"Don\",  \"Rev\",  \"Major\",  \"Sir\",  \"Col\",  \"Capt\",  \"Jonkheer\"]\n",
    "Title_Cat_2 = [\"Mrs\",  \"Mme\",  \"Lady\",  \"Mlle\",  \"the Countess\",  \"Dona\"]\n",
    "Title_Cat_3 = [\"Master\"]\n",
    "Title_Cat_4 = [\"Miss\", \"Ms\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_cat_num = titles.replace(Title_Cat_1, value=\"Adult Male\")\n",
    "title_cat_num = title_cat_num.replace(Title_Cat_2, \"Adult Female\")\n",
    "title_cat_num = title_cat_num.replace(Title_Cat_3, \"Young Boy\")\n",
    "title_cat_num = title_cat_num.replace(Title_Cat_4, \"Young Female\")\n",
    "\n",
    "title_cat_num[(df_full.Sex==\"male\") & (title_cat_num==\"Dr\")]    = \"Adult Male\"\n",
    "title_cat_num[(df_full.Sex==\"female\") & (title_cat_num==\"Dr\")]  = \"Adult Female\"\n",
    "title_cat_num[(df_full.Is_Alone==0) & (title_cat_num==\"Young Female\")] = \"Young Girl\"\n",
    "\n",
    "df_full[GENDER_AGE_CAT] = title_cat_num\n",
    "df_full.Gender_Age_Cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(200).max() / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_2 = df_full.groupby([GENDER_AGE_CAT, PCLASS])\n",
    "df_full[AGE_PREDICT] = grp_2.Age.apply(lambda age: age.fillna(age.mean()))\n",
    "\n",
    "overloaded_group = df_full[(df_full.Pclass==3) & (df_full.Gender_Age_Cat==\"Adult Male\")].Age\n",
    "overloaded_group_index =  overloaded_group[df_full.Age.isna()].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=df_full.notna(), cbar=False, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We have Predicted the missing values, Weather we should use age, is a difficult to answer question...\n",
    "We'll compare the results in later stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In This Part, We'll Encode the Features... <br>\n",
    "\n",
    "* One Hot: Multi Categorical Features\n",
    "* Label Encoding: Binary Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[[*FEATURES_1, *FEATURES_2]].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- * Binary Categorical Features - Survived, Sex, Is_Alone\n",
    "* Multi Categorical Features - Pclass, Embarked, Gender_Age_Cat\n",
    "* Continous Features - Age, SibSp, Parch, Fare, Age_Predict -->\n",
    "\n",
    "|Binary Categorical Features|Multi Categorical Features|Continous Features|\n",
    "|---|---|---|\n",
    "|  ***Survived***, ***Sex***, ***Is_Alone***  |  ***Pclass***, ***Embarked***, ***Gender_Age_Cat***  |  ***Age***, ***SibSp***, ***Parch***, ***Fare***, ***Age_Predict***  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encoder(df_: pd.DataFrame):    \n",
    "    df = df_.copy()\n",
    "    df.Sex = (df.Sex==\"male\").astype(int)\n",
    "    \n",
    "    df[\"Pclass_1\"] = (df.Pclass==1).astype(int)\n",
    "    df[\"Pclass_2\"] = (df.Pclass==2).astype(int)\n",
    "    \n",
    "    df[\"Embarked_From_S\"] = (df.Embarked==\"S\").astype(int)\n",
    "    df[\"Embarked_From_C\"] = (df.Embarked==\"C\").astype(int)\n",
    "    \n",
    "    return df.drop([EMBARKED, PCLASS], axis=1)\n",
    "\n",
    "df_full_encoded = data_encoder(df_full.drop([*FEATURES_USELESS, AGE, NAME, GENDER_AGE_CAT], axis=1))\n",
    "features = df_full_encoded.columns[1:]\n",
    "df_full_encoded.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have Sucessfully Encoded the features and removed the redundent features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dev Test devide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_devide(df_full_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_devide(df_: pd.DataFrame) -> tuple[pd.DataFrame]:\n",
    "    df_train_ = df_[(df_full.Survived.notna())]\n",
    "    df_test_ = df_[(df_full.Survived.isna())]\n",
    "    return df_train_, df_test_\n",
    "\n",
    "def xy_devide(df_: pd.DataFrame) -> tuple[np.ndarray]:\n",
    "    X = df_.iloc[:, 1:].values\n",
    "    y = df_.iloc[:, 0].values\n",
    "    return X, y\n",
    "\n",
    "def train_dev_test_devide(df_: pd.DataFrame, train_size_: float=0.8, random_state_: int=0) -> tuple[np.ndarray]:\n",
    "    df_train_, df_test_ = train_test_devide(df_)\n",
    "    X, y, = xy_devide(df_train_)\n",
    "    X_train_, X_Dev_, y_train_, y_Dev_ = train_test_split(X, y, \n",
    "                                                      train_size=train_size_, \n",
    "                                                      random_state=random_state_)\n",
    "    X_test_, y_test_ = xy_devide(df_test_)\n",
    "    \n",
    "    return X_train_, X_Dev_, X_test_, y_train_, y_Dev_, y_test_\n",
    "\n",
    "\n",
    "X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_devide(df_full_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(map(lambda x: x.shape, [df_full, df_train, df_test])))\n",
    "list(map(lambda x: x.shape, [X_train, X_dev, X_test, y_train, y_dev, y_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In This Part, We'll Scale the Features.\n",
    "\n",
    "* Min-Max Scaling.\n",
    "* Absolute Max Scaling.\n",
    "* Normalize.\n",
    "* Standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_encoded.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have encoded the features and converted them into a numpy object. <br>\n",
    "\n",
    "Let's Scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "indexes = [1, 2, 3, 5]\n",
    "\n",
    "X_train_scaled  = X_train.copy()\n",
    "X_test_scaled   = X_test.copy()\n",
    "X_dev_scaled    = X_dev.copy()\n",
    "\n",
    "X_train_scaled[:, indexes]  = sc.fit_transform(X_train[:, indexes])\n",
    "X_test_scaled[:, indexes]   = sc.transform(X_test[:, indexes])\n",
    "X_dev_scaled[:, indexes]    = sc.transform(X_dev[:, indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled[:1], \"\\n\")\n",
    "print(X_dev_scaled[:1], \"\\n\")\n",
    "print(X_test_scaled[:1], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have Scaled the data, Let's beguin the Basic Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "In this Section, We'll Use Various Models their Hyper Parameters to see which one's better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models: <br>\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Knn\n",
    "3. SVM\n",
    "4. Desecion Tree\n",
    "5. Random Forest\n",
    "6. Gaussian Naive Bayes\n",
    "7. ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "def get_model_score_stats(y_: np.ndarray, y_pred_: np.ndarray) -> tuple[float]:\n",
    "    ac  = accuracy_score(y_, y_pred_)\n",
    "    rcl = recall_score(y_, y_pred_)\n",
    "    pcn = precision_score(y_, y_pred_)\n",
    "    f1  = f1_score(y_, y_pred_)\n",
    "    \n",
    "    return ac, rcl, pcn, f1\n",
    "\n",
    "def get_confussion_matrix(y_: np.ndarray, y_pred_: np.ndarray):\n",
    "    cm = confusion_matrix(y_.tolist(), y_pred_.tolist())\n",
    "    sns.heatmap(cm, annot=True, cmap=\"mako\")\n",
    "    \n",
    "def join_array(y_: np.ndarray, y_pred_: np.ndarray) -> np.ndarray:\n",
    "    m = y_.shape[0]\n",
    "    return np.concatenate(\n",
    "        [y_.reshape(m, 1), y_pred_.reshape(m, 1)], \n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_clf = LogisticRegression()\n",
    "log_reg_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_log = log_reg_clf.predict(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confussion_matrix(y_dev, y_pred_log)\n",
    "\n",
    "plt.title(\"Comfusion Matrix for Logistic Regression\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logreg, recal_logreg, precision_logreg, fi_logreg = get_model_score_stats(y_dev, y_pred_log)\n",
    "accuracy_logreg, recal_logreg, precision_logreg, fi_logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_knn = knn_clf.predict(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confussion_matrix(y_dev, y_pred_knn)\n",
    "\n",
    "plt.title(\"Comfusion Matrix for Logistic Regression\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_knn, recal_knn, precision_knn, fi_knn = get_model_score_stats(y_dev, y_pred_knn)\n",
    "accuracy_knn, recal_knn, precision_knn, fi_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_svm = svm_clf.predict(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confussion_matrix(y_dev, y_pred_svm)\n",
    "\n",
    "plt.title(\"Comfusion Matrix for Logistic Regression\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svm, recal_svm, precision_svm, fi_svm = get_model_score_stats(y_dev, y_pred_svm)\n",
    "accuracy_svm, recal_svm, precision_svm, fi_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dcn_tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "dcn_tree_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_dcn_tree = dcn_tree_clf.predict(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confussion_matrix(y_dev, y_pred_dcn_tree)\n",
    "\n",
    "plt.title(\"Comfusion Matrix for Logistic Regression\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dtree, recal_dtree, precision_dtree, fi_dtree = get_model_score_stats(y_dev, y_pred_dcn_tree)\n",
    "accuracy_dtree, recal_dtree, precision_dtree, fi_dtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rdm_fst_clf = RandomForestClassifier(random_state=0)\n",
    "rdm_fst_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_rdm_fst = rdm_fst_clf.predict(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confussion_matrix(y_dev, y_pred_rdm_fst)\n",
    "\n",
    "plt.title(\"Comfusion Matrix for Logistic Regression\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rforest, recal_rforest, precision_rforest, fi_rforest = get_model_score_stats(y_dev, y_pred_rdm_fst)\n",
    "accuracy_rforest, recal_rforest, precision_rforest, fi_rforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gsn_nb_clf = GaussianNB()\n",
    "gsn_nb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_gsn_nb = gsn_nb_clf.predict(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confussion_matrix(y_dev, y_pred_gsn_nb)\n",
    "\n",
    "plt.title(\"Comfusion Matrix for Logistic Regression\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_nb, recal_nb, precision_nb, fi_nb = get_model_score_stats(y_dev, y_pred_gsn_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "ann_clf = MLPClassifier(solver='adam')\n",
    "ann_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_ann = ann_clf.predict(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confussion_matrix(y_dev, y_pred_ann)\n",
    "\n",
    "plt.title(\"Comfusion Matrix for Logistic Regression\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_score_stats(y_dev, y_pred_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp_df = pd.DataFrame({\n",
    "    \"Model\":            [\"Log Reg\", \"KNN\", \"SVM\", \"Decision Tree\", \"Random Forest\", \"Naive Bayes\"],\n",
    "    \"Accuracy Score\":   [accuracy_logreg, accuracy_knn, accuracy_svm, accuracy_dtree, accuracy_rforest, accuracy_nb],\n",
    "    \"Recal\":            [recal_logreg, recal_knn, recal_svm, recal_dtree, recal_rforest, recal_nb],\n",
    "    \"Precision\":        [precision_logreg, precision_knn, precision_svm, precision_dtree, precision_rforest, precision_nb],\n",
    "    \"F1 Score\":         [fi_logreg, fi_knn, fi_svm, fi_dtree, fi_rforest, fi_nb],\n",
    "    \n",
    "})\n",
    "\n",
    "model_comp_df = model_comp_df.sort_values(by=\"Accuracy Score\", ascending=False)\n",
    "model_comp_df = model_comp_df.set_index(\"Model\")\n",
    "model_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(model_comp_df, annot=True, cmap=\"mako\")\n",
    "plt.title(\"Model Comparison Table\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperpars_lr = {\n",
    "    \"max_iter\":     [20, 50, 100, 150, 200, 500, 1000, 2000],\n",
    "    \"penalty\":      [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "    \"C\":            [100, 10, 1.0, 0.1, 0.01],\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "    \"solver\":       [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],   \n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "log_reg_clf_2 = LogisticRegression()\n",
    "\n",
    "log_reg_grid_search = GridSearchCV(estimator=log_reg_clf_2, \n",
    "                                   param_grid=hyperpars_lr, \n",
    "                                   verbose=1, \n",
    "                                   cv=folds, \n",
    "                                   n_jobs=-1)\n",
    "\n",
    "log_reg_grid_search.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_lg = log_reg_grid_search.best_score_\n",
    "best_hyperparams_lg = log_reg_grid_search.best_params_\n",
    "\n",
    "best_hyperparams_lg, best_score_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_clf_2 = LogisticRegression(**best_hyperparams_lg)\n",
    "log_reg_clf_2.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_log_2 = log_reg_clf_2.predict(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confussion_matrix(y_dev, y_pred_log_2)\n",
    "\n",
    "plt.title(\"Comfusion Matrix for Logistic Regression\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logreg_2, recal_logreg_2, precision_logreg_2, fi_logreg_2 = get_model_score_stats(y_dev, y_pred_log_2)\n",
    "accuracy_logreg_2, recal_logreg_2, precision_logreg_2, fi_logreg_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperpars_knn = {\n",
    "    \"n_neighbors\":  list(range(1, 30)),\n",
    "    \"leaf_size\":    list(range(1, 35)),\n",
    "    \"p\":            [1, 2],\n",
    "    # \"weights\":      [\"uniform\", \"distance\"],   \n",
    "    # \"algorithm\":    [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "\n",
    "knn_grid_search = GridSearchCV(estimator=KNeighborsClassifier(), \n",
    "                                   param_grid=hyperpars_knn, \n",
    "                                   verbose=1, \n",
    "                                   cv=folds, \n",
    "                                   n_jobs=-1)\n",
    "\n",
    "knn_grid_search.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_knn = knn_grid_search.best_score_\n",
    "best_hyperparams_knn = knn_grid_search.best_params_\n",
    "\n",
    "best_hyperparams_knn, best_score_knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_2 = KNeighborsClassifier(**best_hyperparams_knn)\n",
    "knn_clf_2.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_knn_2 = knn_clf_2.predict(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confussion_matrix(y_dev, y_pred_knn_2)\n",
    "\n",
    "plt.title(\"Comfusion Matrix for Logistic Regression\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_knn_2, recal_knn_2, precision_knn_2, fi_knn_2 = get_model_score_stats(y_dev, y_pred_knn_2)\n",
    "accuracy_knn_2, recal_knn_2, precision_knn_2, fi_knn_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperpars_knn = {\n",
    "    \"n_neighbors\":  list(range(1, 30)),\n",
    "    \"leaf_size\":    list(range(1, 35)),\n",
    "    \"p\":            [1, 2],\n",
    "    # \"weights\":      [\"uniform\", \"distance\"],   \n",
    "    # \"algorithm\":    [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "\n",
    "knn_grid_search = GridSearchCV(estimator=KNeighborsClassifier(), \n",
    "                                   param_grid=hyperpars_knn, \n",
    "                                   verbose=1, \n",
    "                                   cv=folds, \n",
    "                                   n_jobs=-1)\n",
    "\n",
    "knn_grid_search.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_knn = knn_grid_search.best_score_\n",
    "best_hyperparams_knn = knn_grid_search.best_params_\n",
    "\n",
    "best_hyperparams_knn, best_score_knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_2 = KNeighborsClassifier(**best_hyperparams_knn)\n",
    "knn_clf_2.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_knn_2 = knn_clf_2.predict(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confussion_matrix(y_dev, y_pred_knn_2)\n",
    "\n",
    "plt.title(\"Comfusion Matrix for Logistic Regression\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_knn_2, recal_knn_2, precision_knn_2, fi_knn_2 = get_model_score_stats(y_dev, y_pred_knn_2)\n",
    "accuracy_knn_2, recal_knn_2, precision_knn_2, fi_knn_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
